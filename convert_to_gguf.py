#!/usr/bin/env python3
"""
Convert the quantized model to GGUF format for efficient CPU inference.
GGUF is optimized for llama.cpp and works great on CPU-only VPS.
"""

import os
import subprocess
import sys
from pathlib import Path

def check_requirements():
    """Check if required tools are installed."""
    print("üîç Checking requirements...")

    try:
        import transformers
        print("‚úÖ transformers installed")
    except ImportError:
        print("‚ùå transformers not installed. Run: pip install transformers")
        return False

    # Check for llama.cpp convert script (new location)
    convert_script = "./llama.cpp/convert_hf_to_gguf.py"
    if os.path.exists(convert_script):
        print("‚úÖ llama.cpp convert script found")
        return convert_script

    # Check for alternative Python conversion
    try:
        import llama_cpp.convert
        print("‚úÖ llama-cpp-python convert module found")
        return "python_convert"
    except ImportError:
        pass

    print("‚ùå No conversion tools found")
    print("\nüì¶ Install with:")
    print("  bash setup_gguf.sh")
    return False

def convert_to_gguf(model_path, output_path):
    """Convert model to GGUF format."""
    print(f"üîÑ Converting {model_path} to GGUF...")

    try:
        # Method 1: Use llama.cpp convert script (most efficient)
        convert_tool = check_requirements()
        if convert_tool and convert_tool != "python_convert" and convert_tool.endswith("convert_hf_to_gguf.py"):
            # Use llama.cpp convert script with correct arguments
            cmd = [
                sys.executable, convert_tool,
                model_path,  # Input model path
                output_path   # Output GGUF path
            ]

            print(f"Running: {' '.join(cmd)}")
            result = subprocess.run(cmd, capture_output=True, text=True)

            if result.returncode == 0:
                print("‚úÖ GGUF conversion successful!")

                # Now quantize to Q4_K_M using llama.cpp quantize tool
                quantize_path = "./llama.cpp/build/bin/quantize"  # CMake build location
                if not os.path.exists(quantize_path):
                    quantize_path = "./llama.cpp/quantize"  # Fallback

                if os.path.exists(quantize_path):
                    quantized_output = output_path.replace(".gguf", "-Q4_K_M.gguf")
                    quantize_cmd = [
                        quantize_path,
                        output_path,
                        quantized_output,
                        "Q4_K_M"
                    ]

                    print(f"Quantizing: {' '.join(quantize_cmd)}")
                    quantize_result = subprocess.run(quantize_cmd, capture_output=True, text=True)

                    if quantize_result.returncode == 0:
                        print("‚úÖ Quantization successful!")
                        # Replace original file with quantized version
                        os.rename(quantized_output, output_path)
                        return True
                    else:
                        print(f"‚ö†Ô∏è  Conversion successful but quantization failed: {quantize_result.stderr}")
                        return True  # Still return True since conversion worked
                else:
                    print("‚ö†Ô∏è  Conversion successful but quantize tool not found")
                    print("üí° You can quantize later with: ./llama.cpp/build/bin/quantize")
                    return True
            else:
                print(f"‚ùå Conversion failed: {result.stderr}")
                return False

        # Method 2: Use llama-cpp-python converter
        try:
            from llama_cpp import convert_hf_to_gguf
            print("Using llama-cpp-python converter...")

            convert_hf_to_gguf.convert_hf_to_gguf(
                model_path=model_path,
                output_path=output_path,
                quantize="Q4_K_M"
            )

            print("‚úÖ GGUF conversion successful!")
            return True

        except ImportError:
            print("‚ùå llama-cpp-python not installed")

        # Method 3: Manual conversion using transformers
        print("Falling back to manual conversion...")
        from transformers import AutoModelForCausalLM, AutoTokenizer
        import torch

        print("Loading model...")
        model = AutoModelForCausalLM.from_pretrained(
            model_path,
            dtype=torch.float16,  # Fixed: use dtype instead of deprecated torch_dtype
            device_map="auto"
        )
        tokenizer = AutoTokenizer.from_pretrained(model_path)

        # This is a simplified conversion - in practice you'd want to use llama.cpp tools
        print("‚ö†Ô∏è  Manual conversion is limited. Consider installing llama.cpp for proper conversion.")
        print("üí° For CPU VPS hosting, GGUF format is still the most efficient option.")

        return False

    except Exception as e:
        print(f"‚ùå Conversion error: {e}")
        return False

def test_gguf_model(gguf_path):
    """Test the converted GGUF model."""
    print(f"üß™ Testing GGUF model: {gguf_path}")

    try:
        from llama_cpp import Llama

        llm = Llama(
            model_path=gguf_path,
            n_ctx=2048,
            n_threads=4,  # Adjust based on your CPU cores
            verbose=False
        )

        # Test prompt
        prompt = """You are a financial analyst. Explain what affects stock market volatility in 2-3 sentences."""

        output = llm(
            prompt,
            max_tokens=256,
            temperature=0.7,
            echo=False
        )

        response = output["choices"][0]["text"].strip()
        print(f"ü§ñ Test response: {response[:200]}...")
        print("‚úÖ GGUF model working!")

        return True

    except ImportError:
        print("‚ùå llama-cpp-python not installed for testing")
        print("Install with: pip install llama-cpp-python")
        return False
    except Exception as e:
        print(f"‚ùå Test failed: {e}")
        return False

def main():
    """Main conversion workflow."""
    print("ü¶ô Converting Qwen3 to GGUF format")
    print("=" * 40)

    # Determine input model
    lora_path = "qwen3-lora"
    quantized_path = "qwen3-quantized-q4km"

    if os.path.exists(quantized_path):
        input_model = quantized_path
        print(f"üìÇ Using quantized model: {input_model}")
    elif os.path.exists(lora_path):
        input_model = lora_path
        print(f"üìÇ Using LoRA model: {input_model}")
    else:
        print("‚ùå No model found! Run training first.")
        return

    # Output path
    output_gguf = "qwen3-gguf-q4km.gguf"

    # Convert to GGUF
    if convert_to_gguf(input_model, output_gguf):
        print(f"üìÅ GGUF model saved as: {output_gguf}")

        # Test the model
        test_gguf_model(output_gguf)

        print("\nüéâ Conversion complete!")
        print("üìä Efficiency comparison:")
        print("  ‚Ä¢ GGUF: Best for CPU VPS (optimized inference)")
        print("  ‚Ä¢ 4-bit: Good for GPU, but needs CUDA")
        print("  ‚Ä¢ Full precision: Slowest, needs lots of RAM")
    else:
        print("\n‚ùå Conversion failed. See errors above.")
        print("\nüí° Alternative: Use the quantized model with transformers:")
        print("  ‚Ä¢ Still efficient for GPU inference")
        print("  ‚Ä¢ Works with vLLM, Text Generation Inference, etc.")

if __name__ == "__main__":
    main()
